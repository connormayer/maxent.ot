---
title: "maxent.ot package demo"
author: "Connor Mayer, Kie Zuraw, and Adeline Tan"
date: "10/21/2022"
output: html_document
---


## Introduction

This presentation will present a brief tutorial on the `maxent.ot` R package we're currently developing. We will show you how to:

* Read tableaux in OTSoft format into R
* Fit a Maximum Entropy OT grammar to the data and evaluate its predictions
* Compare how well different models (i.e., different constraint sets) fit a dataset
* Add in bias/regularization terms to constrain learning

## What is MaxEnt OT?

Maximum Entropy Optimality Theory (henceforth MaxEnt OT; Goldwater & Johnson 2003) is a variant of Optimality Theory (Prince & Smolensky 1993/2004) that uses numeric constraint weights rather than constraint rankings (Pater 2009). Given a set of constraint weights and violation profiles for each output candidate, MaxEnt OT calculates probability distributions over candidates for a given input. This has made it a popular tool for modeling variability in phonological patterns at either the individual or population level (e.g., Hayes & Wilson 2008, Moore-Cantwell & Pater 2016, White 2017, Zuraw & Hayes 2017, Mayer 2021, a.o.).

MaxEnt OT calculates the probability of an output candidate $y$ given an underlying form $x$ as:

$$P(y|x; w) = \frac{1}{Z_w(x)}\exp(-\sum_{k=1}^{m}{w_k f_k(y, x)})$$
$f_k(y, x)$ is the number of violations of constraint $k$ incurred by mapping underlying $x$ to surface $y$. $Z_w(x)$ is a normalization term defined as

$$Z(x) = \sum_{y\in\mathcal{Y}(x)}{\exp(-\sum_{k=1}^{m}{w_k f_k(y, x)})}$$
where $\mathcal{Y}(x)$ is the set of observed surface realizations of input $x$.

The _log likelihood_ of a dataset $D$ given a set of constraint weights $w$ is calculated as 

$$LL_w(D) = \sum_{i=1}^{n}{\ln P(y_i|x_i; w)}$$

where $n$ is the number of data points.

Given a data set consisting of input-output pairs and their corresponding violation profiles, it is straightforward to learn the optimal constraint weights using maximum likelihood estimation (MLE) or maximum a posteriori estimation (MAP; see Hayes & Wilson 2008). Because probabilities can be calculated for each observed output,
different models fit to the same data (that is, models with different constraint weights or sets of constraints) can be compared numerically.

## The data

In this presentation we will walk you through a MaxEnt OT analysis of a simple, fabricated dataset that represents a French-speaking child's acquisition of onset consonant clusters, based loosely on Rose (2002). For this imaginary child:

* cluster simplification is more likely in unstressed syllables
  + e.g. /gry.'o/ vs. /'grav/
* cluster simplification is more likely for s-stop than for stop-liquid
  + e.g. /'stad/ vs. /'grav/

We plot the overall pattern below:

```{r plot, echo=FALSE}
#Create the data frame
simplification <- data.frame(cluster_type=c("ST","TR","ST","TR"),  stress=c("stressed","stressed","unstressed","unstressed"), simplification_rate=c(0.6,0.4,0.9,0.7))

barplot(simplification$simplification_rate, ylab="simplification rate", xlab="cluster type", col=c("blue","blue","gold","gold"), ylim=c(0,1))
axis(1, at = c(0.7, 1.9, 3.1, 4.3), labels = c("s-stop", "stop-liq", "s-stop", "stop-liq"))
legend("topright", fill=c("blue","gold"), legend=c("stressed", "unstressed"))

```

## Loading the library

The first step is to load the maxent.ot library. This needs to be installed from Github since it's not (yet) an official CRAN package.

```{r results=FALSE, message=FALSE, warning=FALSE}
if (!require(devtools)) {
  install.packages("devtools", repos = "http://cran.us.r-project.org")
}
if (!require(maxent.ot)) {
  devtools::install_github("connormayer/maxent.ot")
}
```

## Tableau format

We'll start with a tableau that contains two simple constraints. It's obvious that these constraints can't capture the influences of stress and sonority, but this will let us look at the input data format and set the stage for some model comparison.

* <span style="font-variant:small-caps;">Max</span>: Don't delete segments.
* <span style="font-variant:small-caps;">*Complex</span>: No complex onsets.

Tableaux must be represented in OTSoft format (https://linguistics.ucla.edu/people/hayes/otsoft/). Here's what this looks like. It's not important which separator is used (commas, tabs, etc).

```{r, echo=FALSE, warning=FALSE}
library(flextable) 
tableau <- flextable(read.csv("amp_demo_grammar_base.csv", header=FALSE))
tableau <- delete_part(tableau, part='header')
border_remove(tableau)
```
The first two rows have labels for the constraints.

* Row 1: The full names of the constraitns
* Row 2: Abbreviated constraint names (if desired)

The remaining rows correspond to UR-SR pairs.

The first three columns are unlabeled:

* Column 1: The input names. Note that multiple candidates for the same input are indicated by blank values in this column.
* Column 2: Candidate forms.
* Column 3: Number of observations for each candidate. Note that these must be counts rather than proportions for the model comparisons we'll discuss below to be valid.

The remaining columns correspond to the individual constraints and their violations by each candidate.

# Fitting a MaxEnt grammar and examining its predictions

Fitting a MaxEnt grammar in its most basic form involves computing numeric weights for each constraint that get the model's predictions as close to the observed frequencies in the data by optimizing the log likelihood. We won't talk about how this fitting is done, but see Hayes and Wilson (2008) for a nice description.

We can fit weights using the `optimize_weights` function. In its simplest application, `optimize_weights` takes a single argument, which is the path to the OTSoft file. The `in_sep` argument defines the separator used in the input file. This defaults to tabs (which are the separator used in OTSoft traditionally), but we've used commas here.
```{r}
base_file <- "amp_demo_grammar_base.csv"
base_model <- optimize_weights(base_file, in_sep=',')

# Get the weights of each constraint
base_model$weights

# Get the log likelihood assigned to the training data under these weights
base_model$loglik

# Get the number of free parameters (i.e. number of constraints)
base_model$k

# Get the number of data points
base_model$n
```

## Evaluating model predictions

We can calculate the predictions of a fitted model on some data set using the `predict_probabilities` function. This takes two arguments: the data file, which has the same format as above, and a set of weights. Here we'll use this to get the predicted frequencies for the training data set, but you can also use this on new data provided it uses the same constraints as your trained model.
```{r}

predict_probabilities(base_file, base_model$weights, in_sep=',')
```

We can use `predict_probabilities` to evaluate how well our model generalizes to new data. We can also use it to evaluate the predictions our model makes about the training data, and identify data points that it is particularly bad at accounting for.

Only the three final columns are new here:

* Predicted Probability: The predicted probability of each candidate form under the model.
* Observed Probability: The observed probability in the data.
* Error: The difference between predicted and observed probability. This can be useful for identifying cases that the model does particularly badly on.

Above we can see that it assigns probabilities without considering stress or sonority. This suggests our model might do better if we add some additional constraints.

## Adding in Max-stressed

Let's try adding a new constraint that specifically penalizes deleting segments from stressed syllables. 

* <span style="font-variant:small-caps;">Max-Stressed</span>: Don't delete segments from stressed syllables.

Our updated tableau looks like this:

```{r, echo=FALSE, warning=FALSE}
tableau <- flextable(read.csv("amp_demo_grammar_stressed.csv", header=FALSE))
tableau <- delete_part(tableau, part='header')
border_remove(tableau)
```

Let's fit a new model to it:

```{r}
stressed_file <- "amp_demo_grammar_stressed.csv"
stressed_model <- optimize_weights(stressed_file, in_sep=',')

# Get the weights of each constraint
stressed_model$weights

# Get the log likelihood assigned to the training data under these weights
stressed_model$loglik

# Get the number of free parameters (i.e. number of constraints)
stressed_model$k

# Get the number of data points
stressed_model$n
```

And now let's look at the predictions it makes:

```{r}
predict_probabilities(stressed_file, stressed_model$weights, in_sep=',')
```

This model does a better job of differentiating between frequency of deletion in stressed vs. unstressed syllables, but it doesn't get us the effects of sonority. Let's add one more constraint. 

## Adding in SSP

The last constraint we'll add penalizes onsets that violate the Sonority Sequencing Principle: in this case [sp] onsets.

* <span style="font-variant:small-caps;">SSP</span>: Don't violate the SSP.

Our final tableau looks like this:

```{r, echo=FALSE, warning=FALSE}
tableau <- flextable(read.csv("amp_demo_grammar_full.csv", header=FALSE))
tableau <- delete_part(tableau, part='header')
border_remove(tableau)
```

Let's fit a new model to it:

```{r}
full_file <- "amp_demo_grammar_full.csv"
full_model <- optimize_weights(full_file, in_sep=',')

# Get the weights of each constraint
full_model$weights

# Get the log likelihood assigned to the training data under these weights
full_model$loglik

# Get the number of free parameters (i.e. number of constraints)
full_model$k

# Get the number of data points
full_model$n
```

And now let's look at the predictions it makes:

```{r}
predict_probabilities(full_file, full_model$weights, in_sep=',')
```

Now we're doing pretty well! The model's predicted frequencies capture the qualitative behavior of the observed data quite closely.

# Model comparison

Which of the models we've fit above is the 'best' model? We've qualitatively decided that the full model best reflects the patterns in the data, but how can we quantify this?

One approach is to simply compare the log likelihood scores the models assign to the data.

```{r}
base_model$loglik
stressed_model$loglik
full_model$loglik
```

Higher log likelihoods correspond to greater probability, so the full model is best in the sense that it most accurately predicts the patterns in the data. However, just looking at the log likelihood doesn't take into account the _complexity_ of each model: the base model has two constraints, the stress model has three, and the full model has four. 
Adding more parameters that can be fitted to the data will generally reduce the log likelihood. We want to know whether each new parameter gets us enough 'bang for our buck': is the increase in log likelihood worth the extra complexity of the model?

`maxent.ot` implements several commonly used model comparisons. We won't define these in detail here, but simply show how they can be deployed in `maxent.ot`.

* **The Likelihood Ratio Test** (LRT): This is restricted to comparison between _pairs_ of models with _nested_ constraints: that is, where the constraints of one model are a subset of the constraints of the other.
* **The Akaike Information Criterion** (AIC)
* **AIC with correction for small sample sizes** (AICc)
* **Bayesian Information Criterion** (BIC)

Let's compare our models using the BIC.
```{r}
compare_models(base_model, stressed_model, full_model, method='bic')
```

Lower values of BIC indicate greater preference for a model, and differences of > 4 or so are considered to be meaningful (see, e.g., Burnham & Anderson, 2002). These results tell us that the extra complexity of the two constraints we added is acceptable because of the better fit to the data.

Let's look at a case where the extra complexity of an added constraint doesn't buy us enough to be worth it. We'll add a new constraint to try to coax our predicted probabilities in the right directions.

* <span style="font-variant:small-caps;">DoTheRightThing</span>: Penalize forms that our model overpredicts.

Our final tableau is shown below:

```{r, echo=FALSE, warning=FALSE}
tableau <- flextable(read.csv("amp_demo_grammar_overfit.csv", header=FALSE))
tableau <- delete_part(tableau, part='header')
border_remove(tableau)
```

Let's fit a model to this data and evaluate its predictions.
```{r}
overfit_file <- "amp_demo_grammar_overfit.csv"
overfit_model <- optimize_weights(overfit_file, in_sep=',')
predict_probabilities(overfit_file, overfit_model$weights, in_sep=',')
```

Our predictions are almost identical to the observed frequencies in the data! But at what cost? Let's add the new model to our comparison from above.
```{r}
compare_models(base_model, stressed_model, full_model, overfit_model, method='bic')
```

The new model has a slightly better log likelihood, but the BIC favors the simpler model.

## Adding in bias terms

The models above were all fit using a procedure called _Maximum Likelihood Estimation_ (MLE). This means that the only consideration when fitting the weights is to maximize the likelihood of the data. An alternative way to fit a model to a dataset is called _Maximum A Posteriori Estimation_ (MAP). MAP estimates in the context of a MaxEnt OT model incorporate some belief about the weights prior to seeing any data. This can serve two purposes:

* As _regularization_ to prevent overfitting. Overfitting means that a model does well in predicting the data it is fit to, but generalizes to new data poorly.
* To encode _learning biases_: previous work has used MAP estimates to encode phonological learning biases (e.g., Wilson, 2006; White, 2013). 

`maxent.ot` allows you to specify a normal prior on the weight of each constraint. This means that rather than optimizing the log likelihood, we optimize the following objective function:

$$J_w(D) = LL_w(D) - \sum_{k=1}^{m}{\frac{(w_k - \mu_k)^2}{2\sigma_k^2}}$$

where $m$ is the total number of constraints, $\mu_k$ is the expected value of the $k^{th}$ constraint weight prior to seeing any data, and $\sigma_k$ reflects the prior certainty in the weight of the $k^{th}$ constraint: smaller values of $\sigma_k$ require more data to shift the learned weight away from the expected value.

Let's suppose we get a new set of observations from our French child.
```{r}
tableau <- flextable(read.csv("amp_demo_grammar_new.csv", header=FALSE))
tableau <- delete_part(tableau, part='header')
border_remove(tableau)
```
We can train our model again, but with the same prior on the weights of all constraints. 
```{r}
full_model_map <- optimize_weights(full_file, in_sep=',', mu_scalar=0, sigma_scalar=0.5)
```
Notice that the log likelihood of the training data is worse for the MAP model.
```{r}
full_model$loglik
full_model_map$loglik
```

Let's compare the predictions of each model on the new data.
```{r}
new_file <- "amp_demo_grammar_new.csv"
predict_probabilities(new_file, full_model$weights, in_sep=',')
predict_probabilities(new_file, full_model_map$weights, in_sep=',')
```

Although the new model does a poorer job of fitting the training data, its predictions about the new data are closer to what is observed.

## Additional functionality not covered here

`maxent.ot` has other functionality not covered here, including:

* Saving model predictions to an output file.
* Specifying different bias parameters for individual constraints
* Reading bias parameters from a file rather than specifying them in code
* Changing parameters of the optimizer
* Setting a _temperature_ parameter when predicting new data (e.g., Hayes et al., 2009)

## Planned functionality

We are in the process of implementing some additional functionality in `maxent.ot`, including:

* Cross-validation of different bias parameters
* Reading input from data frames rather than OTSoft-formatted files
